\relax 
\citation{RosenBlat's Perceptron}
\citation{Marlyn Neural Nets}
\citation{Gail Neural Nets}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A Structure of a Neuron}}{1}}
\newlabel{neuron}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Threshold Activation Function}}{1}}
\newlabel{threshold}{{2}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The Adaline and Perceptron Model - here real activation function in the Adaline is actually the quantizer but for comparison purposes (with perceptron model) the units are named as Activation function \& Quantizer }}{1}}
\newlabel{Adaline Perceptron}{{3}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The equation to update the weights in the back-propagation algorithm}}{1}}
\newlabel{backpropagation}{{4}{1}}
\citation{MLPfigure}
\citation{MNIST}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Rectified Linear Activation Function, ReLU }}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Sigmoid Activation Function}}{2}}
\newlabel{relu}{{6}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Multi Layer Perceptron \cite  {MLPfigure}}}{2}}
\newlabel{MLP}{{7}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Data Set Ratio}}{2}}
\newlabel{dataset ratio}{{I}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}The Data}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Sample images of the handwritten digits from the dataset}}{2}}
\newlabel{input}{{8}{2}}
\citation{Google Nets}
\@writefile{toc}{\contentsline {section}{\numberline {III}Implementation and Evaluation of the Network}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Mini-Batch Size}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Processign Elements}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Accuracy of the N/w for varying number of Processing Elements and PCA}}{3}}
\newlabel{processing Elements}{{II}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Learning Rate}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}PCA}{3}}
\citation{relu}
\citation{relu why faster}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Learning Rate vs Learning Curve}}{4}}
\newlabel{learning rate vs learning curve graph}{{9}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces PCA vs Accuracy}}{4}}
\newlabel{pca100}{{10}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-E}}Activation Function}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comparison of ReLU with sigmoid and tanH}}{4}}
\newlabel{relu comparison}{{11}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-F}}Momentum}{4}}
\citation{momentum}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Momentum vs Learning Curve}}{5}}
\newlabel{momVSacc}{{12}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Momentum vs Accuracy}}{5}}
\newlabel{momTable}{{III}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Stochastic Gradient Descent - momentum update \cite  {momentum} }}{5}}
\newlabel{Momentum_eq}{{13}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-G}}Regularization}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Regularization Const vs Learning Curve }}{5}}
\newlabel{regularization graph}{{14}{5}}
\citation{dropout}
\citation{dropout}
\citation{Google Nets}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-H}}Dropout}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces An illustration of dropout \cite  {dropout}}}{6}}
\newlabel{dropout fig}{{15}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Conclusion}{6}}
\citation{MNIST}
\bibcite{MNIST}{1}
\bibcite{RosenBlat's Perceptron}{2}
\bibcite{Marlyn Neural Nets}{3}
\bibcite{Gail Neural Nets}{4}
\bibcite{MLPfigure}{5}
\bibcite{Regularization Parameter}{6}
\bibcite{Google Nets}{7}
\bibcite{momentum}{8}
\bibcite{relu}{9}
\bibcite{relu why faster}{10}
\bibcite{dropout}{11}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Complete List of Configurations and their Comparisons}}{7}}
\newlabel{complete evaluation}{{IV}{7}}
\@writefile{toc}{\contentsline {section}{References}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Implementing Early Stop}}{7}}
\newlabel{Early Stop}{{16}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Learning Curve for 50 Elements in the Hidden Layer}}{8}}
\newlabel{50HL}{{17}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Learning Curves vs No. of Processing Elements}}{8}}
\newlabel{all PE}{{18}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Various Activation Functions for the Training Dataset}}{8}}
\newlabel{Activation Function}{{19}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Results}{9}}
